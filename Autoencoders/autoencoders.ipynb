{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tensorflow.keras.layers import Lambda,Input, Conv2D, Dense, BatchNormalization, Flatten, LeakyReLU, Conv2DTranspose, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "# print(physical_devices)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0],True)\n",
    "\n",
    "tf.random.set_seed(int(time.time()))\n",
    "np.random.seed(int(time.time()))\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "dataset_path = '../Vegetable/'\n",
    "classes = os.listdir(dataset_path)\n",
    "img_rows, img_cols = 28,28\n",
    "\n",
    "\n",
    "for class_id, class_name in enumerate(classes):\n",
    "    folder_path = os.path.join(dataset_path, class_name)\n",
    "    image_files = os.listdir(folder_path)\n",
    "    print(class_id)\n",
    "    for image_file in image_files:\n",
    "        image_path = os.path.join(folder_path, image_file)\n",
    "        img = image.load_img(image_path,target_size=(img_rows, img_cols,3),color_mode='rgb')\n",
    "        img_array = image.img_to_array(img)\n",
    "        data.append(img_array)\n",
    "\n",
    "X = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_temp = train_test_split(X, test_size=0.2, random_state=int(time.time()))\n",
    "x_test,x_val = train_test_split(x_temp, test_size=0.5, random_state=int(time.time()))\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 3).astype('float32')/255.0\n",
    "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 3).astype('float32')/255.0\n",
    "x_val = x_val.reshape(x_val.shape[0], img_rows, img_cols, 3).astype('float32')/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(tf.keras.Model):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(VAE,self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = tf.keras.Sequential(\n",
    "            [\n",
    "                Input(shape=(img_rows,img_cols,3)),\n",
    "                Conv2D(32, kernel_size=3, strides=(2,2), activation='relu'),\n",
    "                Conv2D(64, kernel_size=3, strides=(2,2), activation='relu'),\n",
    "                Flatten(),\n",
    "                Dense(latent_dim + latent_dim)\n",
    "            ]\n",
    "        )\n",
    "        self.decoder = tf.keras.Sequential(\n",
    "            [\n",
    "                Input(shape=(latent_dim,)),\n",
    "                Dense(units=7*7*64, activation=tf.nn.relu),\n",
    "                Reshape(target_shape=(7, 7, 64)),\n",
    "                Conv2DTranspose(filters=128, kernel_size=3, strides=2, padding='same', activation='relu'),\n",
    "                Conv2DTranspose(filters=64, kernel_size=3, strides=2, padding='same', activation='relu'),\n",
    "                # Conv2DTranspose(filters=32, kernel_size=3, strides=2, padding='same', activation='relu'),\n",
    "                Conv2DTranspose(filters=3, kernel_size=3, strides=1, padding='same')\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    @tf.function\n",
    "    def decode(self, z, apply_sigmoid=False):\n",
    "        logits = self.decoder(z)\n",
    "        if apply_sigmoid:\n",
    "            probs = tf.sigmoid(logits)\n",
    "            return probs\n",
    "        return logits\n",
    "    \n",
    "    def encode(self,x):\n",
    "        mean, log_var = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n",
    "        return mean, log_var\n",
    "    \n",
    "    def reparameterize(self, mean,log_var):\n",
    "        eps = tf.random.normal(shape=mean.shape)\n",
    "        return eps*tf.exp(log_var*.5) + mean\n",
    "    \n",
    "    def sample(self, eps=None):\n",
    "        if eps is None:\n",
    "            eps = tf.random.normal(shape=(100,self.latent_dim))\n",
    "        return self.decode(eps, apply_sigmoid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "\n",
    "    \n",
    "def compute_loss(model, x):\n",
    "    mean, logvar = model.encode(x)\n",
    "    z = model.reparameterize(mean, logvar)\n",
    "    x_recon = model.decode(z)\n",
    "    \n",
    "    recon_loss = tf.reduce_mean(tf.square(x - x_recon))\n",
    "    \n",
    "    kl_loss = -0.5 * tf.reduce_sum(1 + logvar - tf.square(mean) - tf.exp(logvar))\n",
    "    \n",
    "    return recon_loss, kl_loss\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, x, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        recon_loss, kl_loss = compute_loss(model, x)\n",
    "        total_loss = recon_loss + kl_loss\n",
    "    \n",
    "    gradients = tape.gradient(total_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return total_loss, recon_loss, kl_loss \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 64\n",
    "loss_values = []\n",
    "\n",
    "vae = VAE(latent_dim=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\somet\\AppData\\Local\\Temp\\ipykernel_9272\\2545861145.py\", line 22, in train_step  *\n        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n    File \"c:\\Users\\somet\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 695, in apply_gradients  **\n        self._create_all_weights(var_list)\n    File \"c:\\Users\\somet\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 959, in _create_all_weights\n        self._create_slots(var_list)\n    File \"c:\\Users\\somet\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py\", line 126, in _create_slots\n        self.add_slot(var, \"m\")\n    File \"c:\\Users\\somet\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 1050, in add_slot\n        weight = tf.Variable(\n\n    ValueError: tf.function only supports singleton tf.Variables created on the first call. Make sure the tf.Variable is only created once or created outside tf.function. See https://www.tensorflow.org/guide/function#creating_tfvariables for more information.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mStarting epoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)  \u001b[39m# New print statement here\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m train_x \u001b[39min\u001b[39;00m x_train:\n\u001b[1;32m----> 4\u001b[0m     total_loss, recon_loss, kl_loss \u001b[39m=\u001b[39m train_step(vae, tf\u001b[39m.\u001b[39;49mexpand_dims(train_x,\u001b[39m0\u001b[39;49m), optimizer)\n\u001b[0;32m      5\u001b[0m loss_values\u001b[39m.\u001b[39mappend(recon_loss\u001b[39m.\u001b[39mnumpy())\n\u001b[0;32m      6\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFinished epoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m) \n",
      "File \u001b[1;32mc:\\Users\\somet\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileomlg_6f8.py:14\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step\u001b[1;34m(model, x, optimizer)\u001b[0m\n\u001b[0;32m     12\u001b[0m     total_loss \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mld(recon_loss) \u001b[39m+\u001b[39m ag__\u001b[39m.\u001b[39mld(kl_loss)\n\u001b[0;32m     13\u001b[0m gradients \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tape)\u001b[39m.\u001b[39mgradient, (ag__\u001b[39m.\u001b[39mld(total_loss), ag__\u001b[39m.\u001b[39mld(model)\u001b[39m.\u001b[39mtrainable_variables), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m---> 14\u001b[0m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(optimizer)\u001b[39m.\u001b[39;49mapply_gradients, (ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(\u001b[39mzip\u001b[39;49m), (ag__\u001b[39m.\u001b[39;49mld(gradients), ag__\u001b[39m.\u001b[39;49mld(model)\u001b[39m.\u001b[39;49mtrainable_variables), \u001b[39mNone\u001b[39;49;00m, fscope),), \u001b[39mNone\u001b[39;49;00m, fscope)\n\u001b[0;32m     15\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     16\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\somet\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py:695\u001b[0m, in \u001b[0;36mOptimizerV2.apply_gradients\u001b[1;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[0;32m    692\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mname_scope(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name):\n\u001b[0;32m    693\u001b[0m     \u001b[39m# Create iteration if necessary.\u001b[39;00m\n\u001b[0;32m    694\u001b[0m     \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39minit_scope():\n\u001b[1;32m--> 695\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_all_weights(var_list)\n\u001b[0;32m    697\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m grads_and_vars:\n\u001b[0;32m    698\u001b[0m         \u001b[39m# Distribution strategy does not support reducing an empty list\u001b[39;00m\n\u001b[0;32m    699\u001b[0m         \u001b[39m# of gradients\u001b[39;00m\n\u001b[0;32m    700\u001b[0m         \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mno_op()\n",
      "File \u001b[1;32mc:\\Users\\somet\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py:959\u001b[0m, in \u001b[0;36mOptimizerV2._create_all_weights\u001b[1;34m(self, var_list)\u001b[0m\n\u001b[0;32m    957\u001b[0m _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miterations\n\u001b[0;32m    958\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_hypers()\n\u001b[1;32m--> 959\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_slots(var_list)\n\u001b[0;32m    960\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_slots_for_sharded_variables(var_list)\n",
      "File \u001b[1;32mc:\\Users\\somet\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:126\u001b[0m, in \u001b[0;36mAdam._create_slots\u001b[1;34m(self, var_list)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_create_slots\u001b[39m(\u001b[39mself\u001b[39m, var_list):\n\u001b[0;32m    123\u001b[0m     \u001b[39m# Create slots for the first and second moments.\u001b[39;00m\n\u001b[0;32m    124\u001b[0m     \u001b[39m# Separate for-loops to respect the ordering of slot variables from v1.\u001b[39;00m\n\u001b[0;32m    125\u001b[0m     \u001b[39mfor\u001b[39;00m var \u001b[39min\u001b[39;00m var_list:\n\u001b[1;32m--> 126\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_slot(var, \u001b[39m\"\u001b[39;49m\u001b[39mm\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    127\u001b[0m     \u001b[39mfor\u001b[39;00m var \u001b[39min\u001b[39;00m var_list:\n\u001b[0;32m    128\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_slot(var, \u001b[39m\"\u001b[39m\u001b[39mv\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\somet\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py:1050\u001b[0m, in \u001b[0;36mOptimizerV2.add_slot\u001b[1;34m(self, var, slot_name, initializer, shape)\u001b[0m\n\u001b[0;32m   1039\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1040\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mTrying to create optimizer slot variable under the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1041\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mscope for tf.distribute.Strategy (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m), which is \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1046\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39moutside the scope.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(strategy, var)\n\u001b[0;32m   1047\u001b[0m         )\n\u001b[0;32m   1049\u001b[0m     \u001b[39mwith\u001b[39;00m strategy\u001b[39m.\u001b[39mextended\u001b[39m.\u001b[39mcolocate_vars_with(var):\n\u001b[1;32m-> 1050\u001b[0m         weight \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mVariable(\n\u001b[0;32m   1051\u001b[0m             name\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mvar\u001b[39m.\u001b[39;49m_shared_name\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mslot_name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1052\u001b[0m             dtype\u001b[39m=\u001b[39;49mvar\u001b[39m.\u001b[39;49mdtype,\n\u001b[0;32m   1053\u001b[0m             trainable\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m   1054\u001b[0m             initial_value\u001b[39m=\u001b[39;49minitial_value,\n\u001b[0;32m   1055\u001b[0m         )\n\u001b[0;32m   1056\u001b[0m backend\u001b[39m.\u001b[39mtrack_variable(weight)\n\u001b[0;32m   1057\u001b[0m slot_dict[slot_name] \u001b[39m=\u001b[39m weight\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\somet\\AppData\\Local\\Temp\\ipykernel_9272\\2545861145.py\", line 22, in train_step  *\n        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n    File \"c:\\Users\\somet\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 695, in apply_gradients  **\n        self._create_all_weights(var_list)\n    File \"c:\\Users\\somet\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 959, in _create_all_weights\n        self._create_slots(var_list)\n    File \"c:\\Users\\somet\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py\", line 126, in _create_slots\n        self.add_slot(var, \"m\")\n    File \"c:\\Users\\somet\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 1050, in add_slot\n        weight = tf.Variable(\n\n    ValueError: tf.function only supports singleton tf.Variables created on the first call. Make sure the tf.Variable is only created once or created outside tf.function. See https://www.tensorflow.org/guide/function#creating_tfvariables for more information.\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(f\"Starting epoch {epoch}\")  # New print statement here\n",
    "    for train_x in x_train:\n",
    "        total_loss, recon_loss, kl_loss = train_step(vae, tf.expand_dims(train_x,0), optimizer)\n",
    "    loss_values.append(recon_loss.numpy())\n",
    "    print(f\"Finished epoch {epoch}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_values)\n",
    "plt.title('Reconstruction Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
